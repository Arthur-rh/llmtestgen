from __future__ import annotations

from enum import Enum
from pathlib import Path
from typing import Callable, List, Optional

import json
from pydantic import BaseModel, Field, ValidationError

from llmtestgen.services.spec_analyser.parse_router_normalizer import (
    NormalizedSpec,
    parse_spec,
)
from llmtestgen.wrappers.git_repository import GitRepository
from llmtestgen.core.utils_errors import SpecParsingError


# ==============================================================================
# Models for test specifications
# ==============================================================================


class TestCase(BaseModel):
    """Single test case generated by the LLM."""
    id: Optional[str] = None
    requirement: Optional[str] = None  # raw requirement text or ID reference
    description: str
    preconditions: List[str] = Field(default_factory=list)
    steps: List[str] = Field(default_factory=list)
    expected_result: str
    target_code_elements: List[str] = Field(default_factory=list)  # file paths, functions, etc.


class TestSpecification(BaseModel):
    """Global test specification generated from a spec + code."""
    spec_source_path: str
    test_cases: List[TestCase] = Field(default_factory=list)
    llm_model: Optional[str] = None
    llm_raw_response: Optional[str] = None


# ==============================================================================
# Code context configuration
# ==============================================================================


class CodeContextLevel(str, Enum):
    """How much code context to provide to the LLM."""
    NONE = "none"
    FILE_LIST = "file_list"           # only list of Python files
    FILE_SNIPPETS = "file_snippets"   # list + truncated contents
    FULL = "full"                     # full contents of all Python files (careful on big repos)


def build_python_code_context(
    repo: GitRepository,
    *,
    level: CodeContextLevel = CodeContextLevel.FILE_SNIPPETS,
    max_files: int = 20,
    max_chars_per_file: int = 4000,
) -> str:
    """Build a textual context of Python files for the LLM.

    - Filters files to .py
    - Depending on `level`, includes only filenames or also contents.
    - Truncates per-file content to avoid huge prompts.
    """
    repo.open()
    all_files = repo.list_files()
    py_files = [f for f in all_files if f.endswith(".py")]

    if level == CodeContextLevel.NONE or not py_files:
        return ""

    lines: List[str] = []
    lines.append("Project code overview (Python only):")

    if level in (CodeContextLevel.FILE_LIST, CodeContextLevel.FILE_SNIPPETS, CodeContextLevel.FULL):
        lines.append("\nPython files:")
        for path in py_files:
            lines.append(f"- {path}")

    if level in (CodeContextLevel.FILE_SNIPPETS, CodeContextLevel.FULL):
        lines.append("\nDetailed code context:")
        limit = None if level == CodeContextLevel.FULL else max_files

        for idx, path in enumerate(py_files):
            if limit is not None and idx >= limit:
                lines.append(
                    f"\n[Truncated: only first {limit} Python files included in detailed context.]"
                )
                break

            try:
                content = repo.get_file_contents(path)
            except Exception:
                continue

            if level == CodeContextLevel.FILE_SNIPPETS and len(content) > max_chars_per_file:
                content = content[:max_chars_per_file] + "\n\n# [Truncated content...]"

            lines.append("\n" + "=" * 80)
            lines.append(f"# FILE: {path}")
            lines.append("=" * 80)
            lines.append(content)

    return "\n".join(lines).strip()


# ==============================================================================
# Test specification generator (LLM-based)
# ==============================================================================


class TestSpecGenerator:
    """Generate test specifications from a normalized spec and Python code."""

    def __init__(
        self,
        send_prompt_fn: Callable[..., str],
        *,
        model: Optional[str] = None,
        api_key: Optional[str] = None,
        code_context_level: CodeContextLevel = CodeContextLevel.FILE_SNIPPETS,
        max_files: int = 20,
        max_chars_per_file: int = 4000,
    ) -> None:
        """
        Args:
            send_prompt_fn: function with signature:
                send_prompt(prompt: str, *, api_key, model, system_prompt, **kwargs) -> str
            model: default LLM model name
            api_key: API key for the LLM backend
            code_context_level: how much code to expose (none, file list, snippets, full)
            max_files: limit the number of Python files included in the context
            max_chars_per_file: truncate large files when using FILE_SNIPPETS
        """
        self.send_prompt_fn = send_prompt_fn
        self.model = model
        self.api_key = api_key
        self.code_context_level = code_context_level
        self.max_files = max_files
        self.max_chars_per_file = max_chars_per_file

    # ------------------------------------------------------------------
    # Public API
    # ------------------------------------------------------------------

    def generate(
        self,
        spec: NormalizedSpec,
        repo: Optional[GitRepository] = None,
    ) -> TestSpecification:
        """Generate a test specification from a spec + optional code repository."""
        code_context = ""
        if repo is not None and self.code_context_level != CodeContextLevel.NONE:
            code_context = build_python_code_context(
                repo,
                level=self.code_context_level,
                max_files=self.max_files,
                max_chars_per_file=self.max_chars_per_file,
            )

        system_prompt = self._build_system_prompt()
        user_prompt = self._build_user_prompt(spec, code_context)

        response_text = self.send_prompt_fn(
            user_prompt,
            api_key=self.api_key,
            model=self.model,
            system_prompt=system_prompt,
        )

        return self._parse_llm_response(
            response_text=response_text,
            spec=spec,
        )

    # ------------------------------------------------------------------
    # Prompt construction
    # ------------------------------------------------------------------

    def _build_system_prompt(self) -> str:
        """Explain to the LLM how to behave and what JSON to return."""
        return (
            "You are an expert software test engineer. "
            "Your task is to design high-quality test cases for a Python project.\n\n"
            "Given a project specification and an optional view of the codebase, "
            "you must produce a structured list of test cases as JSON.\n\n"
            "Return ONLY a JSON object with this exact structure:\n"
            "{\n"
            '  "test_cases": [\n'
            "    {\n"
            '      "id": string or null,\n'
            '      "requirement": string or null,\n'
            '      "description": string,\n'
            '      "preconditions": [string, ...],\n'
            '      "steps": [string, ...],\n'
            '      "expected_result": string,\n'
            '      "target_code_elements": [string, ...]\n'
            "    },\n"
            "    ...\n"
            "  ]\n"
            "}\n\n"
            "Do not include explanations, comments, or additional fields. "
            "Do not wrap the JSON in code fences."
        )

    def _build_user_prompt(self, spec: NormalizedSpec, code_context: str) -> str:
        parts: List[str] = []

        parts.append("Project specification (normalized):")
        if spec.title:
            parts.append(f"\nTitle: {spec.title}")
        parts.append(f"\nSource path: {spec.source_path}")

        if spec.sections:
            parts.append("\n\nSections:")
            for name, content in spec.sections.items():
                parts.append(f"\n### {name}\n{content}")

        if spec.requirements:
            parts.append("\n\nExtracted requirements:")
            for req in spec.requirements:
                parts.append(f"- {req}")

        if spec.acceptance_criteria:
            parts.append("\n\nAcceptance criteria:")
            for ac in spec.acceptance_criteria:
                parts.append(f"- {ac}")

        if spec.examples:
            parts.append("\n\nExamples:")
            for ex in spec.examples:
                parts.append(f"- {ex}")

        # Optionally include raw text (can be large; you can decide to omit or keep)
        # parts.append("\n\nRaw specification text:\n")
        # parts.append(spec.raw_text)

        if code_context:
            parts.append("\n\nCode context (Python project):\n")
            parts.append(code_context)
        else:
            parts.append("\n\nNo code context provided. "
                         "Design tests only from the specification above.")

        parts.append(
            "\n\nUsing the information above, generate a comprehensive list of test cases "
            "that validate the expected behavior of the system."
        )

        return "\n".join(parts)

    # ------------------------------------------------------------------
    # LLM response parsing
    # ------------------------------------------------------------------

    def _parse_llm_response(
        self,
        *,
        response_text: str,
        spec: NormalizedSpec,
    ) -> TestSpecification:
        """Parse the JSON returned by the LLM into a TestSpecification."""
        try:
            data = json.loads(response_text)
        except json.JSONDecodeError as err:
            raise ValueError(
                "LLM returned invalid JSON when generating test specification.\n"
                f"Raw response:\n{response_text}"
            ) from err

        raw_cases = data.get("test_cases") or []
        test_cases: List[TestCase] = []

        if not isinstance(raw_cases, list):
            raise ValueError("LLM JSON must contain a 'test_cases' array.")

        for idx, tc in enumerate(raw_cases):
            if not isinstance(tc, dict):
                continue
            try:
                test_case = TestCase(
                    id=tc.get("id"),
                    requirement=tc.get("requirement"),
                    description=tc.get("description") or f"Test case #{idx + 1}",
                    preconditions=tc.get("preconditions") or [],
                    steps=tc.get("steps") or [],
                    expected_result=tc.get("expected_result") or "",
                    target_code_elements=tc.get("target_code_elements") or [],
                )
                test_cases.append(test_case)
            except ValidationError:
                # Skip invalid entries rather than failing the whole generation
                continue

        return TestSpecification(
            spec_source_path=spec.source_path,
            test_cases=test_cases,
            llm_model=self.model,
            llm_raw_response=response_text,
        )


# ==============================================================================
# High-level convenience function
# ==============================================================================


def generate_test_spec_from_paths(
    spec_path: str | Path,
    repo_source: Optional[str],
    *,
    send_prompt_fn,
    model: Optional[str] = None,
    api_key: Optional[str] = None,
    code_context_level: CodeContextLevel = CodeContextLevel.FILE_SNIPPETS,
    max_files: int = 20,
    max_chars_per_file: int = 4000,
    use_llm_for_spec: bool = False,
    llm_fallback_for_spec: bool = False,
) -> TestSpecification:
    """End-to-end helper: parse spec file, optionally open repo, and generate tests.

    This wires together:
    - Spec parsing via `parse_spec`
    - Git repository context via `GitRepository`
    - LLM-based test spec generation via `TestSpecGenerator`
    """

    # 1) Parse the specification into NormalizedSpec
    parse_result = parse_spec(
        spec_path,
        send_prompt_fn=send_prompt_fn,
        model=model,
        api_key=api_key,
        use_llm=use_llm_for_spec,
        llm_fallback=llm_fallback_for_spec,
    )
    spec = parse_result.spec

    # 2) Optionally open the repository
    repo: Optional[GitRepository] = None
    if repo_source is not None:
        repo = GitRepository(repo_source)

    # 3) Generate test specification
    generator = TestSpecGenerator(
        send_prompt_fn,
        model=model,
        api_key=api_key,
        code_context_level=code_context_level,
        max_files=max_files,
        max_chars_per_file=max_chars_per_file,
    )

    return generator.generate(spec, repo)
